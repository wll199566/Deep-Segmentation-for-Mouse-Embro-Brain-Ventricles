{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "import pathlib\n",
    "import nibabel as nib\n",
    "from skimage import measure\n",
    "\n",
    "from Extract_mouse_data import Mouse_sub_volumes\n",
    "from data_augmentation import Rotate, Flip\n",
    "from seg_net import Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Function and class here\n",
    "def get_subbox_idx(input_idx,input_all_data):\n",
    "    sub_box=[]\n",
    "    pos_num=0\n",
    "    for idx in input_idx:\n",
    "        for i in range(len(input_all_data[idx][0])):\n",
    "            sub_box.append((idx,input_all_data[idx][0][i]))\n",
    "        pos_num+=len(input_all_data[idx][0])\n",
    "    print(pos_num)\n",
    "    print(len(sub_box))\n",
    "    return sub_box\n",
    "\n",
    "def save_nii(img, pred, lbl, save_num, score):\n",
    "    img_nft = nib.Nifti1Image(np.squeeze(img),np.eye(4))\n",
    "    pred_nft = nib.Nifti1Image(np.squeeze(pred), np.eye(4))\n",
    "    lbl_nft = nib.Nifti1Image(np.squeeze(lbl), np.eye(4))\n",
    "    \n",
    "    nib.save(img_nft, './predict/img{}.nii'.format(save_num))\n",
    "    nib.save(pred_nft,'./predict/pred{}_{:.3f}.nii'.format(save_num,score))\n",
    "    nib.save(lbl_nft, './predict/label{}.nii'.format(save_num))\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i_batch, sample_batched in enumerate(train_loader):\n",
    "        inputs, labels = sample_batched['image'], sample_batched['label'] \n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        del inputs\n",
    "        \n",
    "        loss = criterion(outputs, labels.float())\n",
    "        \n",
    "        del labels \n",
    "        del sample_batched\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        del loss\n",
    "        print_interval = 1\n",
    "        if i_batch % print_interval == 0:\n",
    "            print(\"epoch {}, batch {}, current loss {}\".format(epoch+1,i_batch,running_loss/print_interval))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    \n",
    "def test(model1, model2, device, full_img, full_fil_img, full_label, right_corner, save_num, box_size=128, save_thresh = 0.92):\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    with torch.no_grad():\n",
    "        input_img = np.zeros((1,1,box_size,box_size,box_size), np.float32)\n",
    "        input_img[0,0,...] = full_img[right_corner[0]-box_size:right_corner[0],\n",
    "                                      right_corner[1]-box_size:right_corner[1],\n",
    "                                      right_corner[2]-box_size:right_corner[2]]\n",
    "        #input_img[0,1,...] = full_fil_img[right_corner[0]-box_size:right_corner[0],\n",
    "        #                                  right_corner[1]-box_size:right_corner[1],\n",
    "        #                                  right_corner[2]-box_size:right_corner[2]]\n",
    "        \n",
    "        y_hat1 = model1(torch.from_numpy(input_img).to(device))\n",
    "        y_hat2 = model2(torch.from_numpy(input_img).to(device))\n",
    "        y_hat1[y_hat1 >  save_thresh] = 1.0\n",
    "        y_hat1[y_hat1 <= save_thresh] = 0.0\n",
    "        y_hat2[y_hat2 >  save_thresh] = 1.0\n",
    "        y_hat2[y_hat2 <= save_thresh] = 0.0\n",
    "        y_hat = y_hat1 + y_hat2\n",
    "        y_hat[y_hat > 0] = 1.0\n",
    "        y_predict = np.zeros((np.shape(full_img)),np.uint8)\n",
    "        y_predict[right_corner[0]-box_size:right_corner[0],\n",
    "                  right_corner[1]-box_size:right_corner[1],\n",
    "                  right_corner[2]-box_size:right_corner[2]] = np.squeeze(y_hat.cpu().numpy())\n",
    "        y_predict_component = measure.label(y_predict)\n",
    "        component_num = np.unique(y_predict_component)\n",
    "        for current_component in range(1,len(component_num)):\n",
    "            if np.sum(y_predict_component == current_component) < 300:\n",
    "                y_predict[y_predict_component == current_component] = 0\n",
    "        # score binary output\n",
    "        score = 2*np.sum(y_predict*full_label) / (np.sum(y_predict) + np.sum(full_label))\n",
    "        save_nii(full_img + 0.5, y_predict, full_label, save_num, score)\n",
    "        #save_nii(input_img[0,0,...] + 0.5, y_hat.cpu().numpy(), full_label[right_corner[0]-box_size:right_corner[0],\n",
    "        #                              right_corner[1]-box_size:right_corner[1],\n",
    "        #                              right_corner[2]-box_size:right_corner[2]], save_num, score)\n",
    "        contain_ratio = (np.sum(full_label[right_corner[0]-box_size:right_corner[0],\n",
    "                                          right_corner[1]-box_size:right_corner[1],\n",
    "                                          right_corner[2]-box_size:right_corner[2]]) / (np.sum(full_label) + 0.0001))\n",
    "        \n",
    "        print('Img_num: {}, contain_ratio: {}, f-score: {}'.format(save_num, contain_ratio, score))\n",
    "        print('predict_bv_pixel: {}, true_bv_pixel: {}'.format(np.sum(y_predict),np.sum(full_label)))\n",
    "        del input_img, y_predict\n",
    "    return score\n",
    "\n",
    "def test_ensemble(model_list, device, test_loader):\n",
    "    for i in range(len(model_list)):\n",
    "        model_list[i].eval()\n",
    "    #model3.eval()\n",
    "    correct_num = 0\n",
    "    total_num = 0\n",
    "    positive_correct=0\n",
    "    positive_num=0\n",
    "    negative_correct=0\n",
    "    negative_num=0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i_batch, sample_batched in enumerate(test_loader):\n",
    "            inputs, labels = sample_batched['image'], sample_batched['label']  \n",
    "            inputs = inputs.to(device)\n",
    "            # forward + backward + optimize\n",
    "            outputs = torch.nn.functional.softmax(model_list[0](inputs),dim=1)\n",
    "            for i in range(1,len(model_list)):\n",
    "                outputs += torch.nn.functional.softmax(model_list[i](inputs),dim=1)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_num+=np.sum(predicted.cpu().numpy()==labels.numpy())\n",
    "            total_num+=len(labels)\n",
    "            positive_correct+=np.sum(predicted.cpu().numpy()*labels.numpy())\n",
    "            positive_num+=np.sum(labels.numpy())\n",
    "            negative_correct+=np.sum((1-predicted.cpu().numpy())*(1-labels.numpy()))\n",
    "            negative_num+=np.sum(1-labels.numpy())\n",
    "            \n",
    "    print('total_num:{}, test accuracy:{}, positive_acc:{}, negative_acc:{}'.format(total_num,\n",
    "                                                                                   correct_num/total_num,\n",
    "                                                                                    positive_correct/positive_num,\n",
    "                                                                                    negative_correct/negative_num\n",
    "                                                                                    ))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_data(data_path):\n",
    "    with open(data_path,'rb') as f:\n",
    "        #(pos_subvolumes,img2,filtered_img2,img_label2,data_dic[i][0])\n",
    "        all_data = pickle.load(f)\n",
    "    f.close()\n",
    "    return all_data\n",
    "\n",
    "def dice_loss(source, target):\n",
    "    # flatten images to vectors so score can be computed with vector ops \n",
    "    batch_size, num_channels = source.size(0), source.size(1)\n",
    "    \n",
    "    smooth = 1e-4\n",
    "    s = source.view(batch_size, -1)\n",
    "    t = target.view(batch_size, -1)\n",
    "    \n",
    "    # positive class Dice\n",
    "    intersect   =     (s * t).sum(dim=1)\n",
    "    cardinality =     s.sum(dim=1) + t.sum(dim=1)\n",
    "    \n",
    "    dsc_p = (2*intersect+smooth)/(cardinality+smooth)\n",
    "    \n",
    "    return 1-dsc_p.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kernel_size': 7, 'padding': 3, 'dilation': 1, 'stride': 1}\n",
      "{'kernel_size': 7, 'padding': 3, 'dilation': 1, 'stride': 1}\n",
      "{'kernel_size': 7, 'padding': 3, 'dilation': 1, 'stride': 1}\n",
      "{'kernel_size': 7, 'padding': 3, 'dilation': 1, 'stride': 2}\n",
      "{'kernel_size': 7, 'padding': 3, 'dilation': 1, 'stride': 2}\n",
      "{'kernel_size': 7, 'padding': 3, 'dilation': 1, 'stride': 1}\n",
      "{'kernel_size': 7, 'padding': 3, 'dilation': 1, 'stride': 1}\n",
      "{'kernel_size': 7, 'padding': 3, 'dilation': 1, 'stride': 1}\n",
      "{'kernel_size': 7, 'padding': 3, 'dilation': 1, 'stride': 1}\n",
      "{'kernel_size': 7, 'padding': 3, 'dilation': 1, 'stride': 1}\n",
      "{'kernel_size': 7, 'padding': 3, 'dilation': 1, 'stride': 2}\n",
      "{'kernel_size': 7, 'padding': 3, 'dilation': 1, 'stride': 2}\n",
      "{'kernel_size': 7, 'padding': 3, 'dilation': 1, 'stride': 1}\n",
      "{'kernel_size': 7, 'padding': 3, 'dilation': 1, 'stride': 1}\n",
      "Let's use 2 GPUs!\n",
      "There are 1990129 parameters in the model\n",
      "Img_num: 0, contain_ratio: 0.9999999972739417, f-score: 0.9499143778789608\n",
      "predict_bv_pixel: 38648, true_bv_pixel: 36683\n",
      "Img_num: 1, contain_ratio: 0.9999999969075671, f-score: 0.9296588542430554\n",
      "predict_bv_pixel: 35845, true_bv_pixel: 32337\n",
      "Img_num: 2, contain_ratio: 0.9999999974290417, f-score: 0.9488530496908972\n",
      "predict_bv_pixel: 42145, true_bv_pixel: 38896\n",
      "Img_num: 3, contain_ratio: 0.999999996840642, f-score: 0.9202921562400762\n",
      "predict_bv_pixel: 34477, true_bv_pixel: 31652\n",
      "Img_num: 4, contain_ratio: 0.9999999966666666, f-score: 0.9397190293742018\n",
      "predict_bv_pixel: 32640, true_bv_pixel: 30000\n",
      "Img_num: 5, contain_ratio: 0.9999999972947383, f-score: 0.9432348672080342\n",
      "predict_bv_pixel: 39508, true_bv_pixel: 36965\n",
      "Img_num: 6, contain_ratio: 0.9999999961101602, f-score: 0.9141516245487364\n",
      "predict_bv_pixel: 29692, true_bv_pixel: 25708\n",
      "Img_num: 7, contain_ratio: 0.9999999947005829, f-score: 0.8990097983306548\n",
      "predict_bv_pixel: 19708, true_bv_pixel: 18870\n",
      "Img_num: 8, contain_ratio: 0.9999999945652174, f-score: 0.896474358974359\n",
      "predict_bv_pixel: 19040, true_bv_pixel: 18400\n",
      "Img_num: 9, contain_ratio: 0.9999999951545692, f-score: 0.9232790815620822\n",
      "predict_bv_pixel: 22740, true_bv_pixel: 20638\n",
      "Img_num: 10, contain_ratio: 0.9999999959916627, f-score: 0.894494767920625\n",
      "predict_bv_pixel: 24841, true_bv_pixel: 24948\n",
      "Img_num: 11, contain_ratio: 0.9999999958139729, f-score: 0.9116442605997932\n",
      "predict_bv_pixel: 24461, true_bv_pixel: 23889\n",
      "Img_num: 12, contain_ratio: 0.9999999961588691, f-score: 0.9167025089605735\n",
      "predict_bv_pixel: 29766, true_bv_pixel: 26034\n",
      "Img_num: 13, contain_ratio: 0.9999999954450214, f-score: 0.8793825168970657\n",
      "predict_bv_pixel: 27019, true_bv_pixel: 21954\n",
      "Img_num: 14, contain_ratio: 0.9999999954654695, f-score: 0.8782595925902617\n",
      "predict_bv_pixel: 25614, true_bv_pixel: 22053\n",
      "Img_num: 15, contain_ratio: 0.9999999964248686, f-score: 0.8885680400144382\n",
      "predict_bv_pixel: 30208, true_bv_pixel: 27971\n",
      "Img_num: 16, contain_ratio: 0.9999999959628583, f-score: 0.8471869068509831\n",
      "predict_bv_pixel: 31076, true_bv_pixel: 24770\n",
      "Img_num: 17, contain_ratio: 0.9984913766204435, f-score: 0.9286142159799136\n",
      "predict_bv_pixel: 41341, true_bv_pixel: 37120\n",
      "Img_num: 18, contain_ratio: 0.9999999942653974, f-score: 0.8880248833592534\n",
      "predict_bv_pixel: 21142, true_bv_pixel: 17438\n",
      "Img_num: 19, contain_ratio: 0.9999999964730364, f-score: 0.914434425142554\n",
      "predict_bv_pixel: 31975, true_bv_pixel: 28353\n",
      "Img_num: 20, contain_ratio: 0.9999999952201137, f-score: 0.8924076147816349\n",
      "predict_bv_pixel: 23729, true_bv_pixel: 20921\n",
      "Img_num: 21, contain_ratio: 0.9999999956347128, f-score: 0.897321251928936\n",
      "predict_bv_pixel: 27638, true_bv_pixel: 22908\n",
      "Img_num: 22, contain_ratio: 0.9999999951557428, f-score: 0.8183047611471301\n",
      "predict_bv_pixel: 29639, true_bv_pixel: 20643\n",
      "Img_num: 23, contain_ratio: 0.9999999956369983, f-score: 0.9002477559806669\n",
      "predict_bv_pixel: 26322, true_bv_pixel: 22920\n",
      "Img_num: 24, contain_ratio: 0.9999999961292819, f-score: 0.8041787456692356\n",
      "predict_bv_pixel: 30737, true_bv_pixel: 25835\n",
      "Img_num: 25, contain_ratio: 0.9999999960168884, f-score: 0.8930986823919653\n",
      "predict_bv_pixel: 29159, true_bv_pixel: 25106\n",
      "Img_num: 26, contain_ratio: 0.9999999954576425, f-score: 0.8916839611764459\n",
      "predict_bv_pixel: 25688, true_bv_pixel: 22015\n",
      "Img_num: 27, contain_ratio: 0.9999999950477888, f-score: 0.8733400494550783\n",
      "predict_bv_pixel: 23483, true_bv_pixel: 20193\n",
      "Img_num: 28, contain_ratio: 0.9999999965381153, f-score: 0.9084149934900295\n",
      "predict_bv_pixel: 29486, true_bv_pixel: 28886\n",
      "Img_num: 29, contain_ratio: 0.9917507359540015, f-score: 0.8638209742058689\n",
      "predict_bv_pixel: 20329, true_bv_pixel: 16850\n",
      "Img_num: 30, contain_ratio: 0.999999995153395, f-score: 0.7975916210094283\n",
      "predict_bv_pixel: 31020, true_bv_pixel: 20633\n",
      "Img_num: 31, contain_ratio: 0.9999999929408444, f-score: 0.8056333200022853\n",
      "predict_bv_pixel: 20840, true_bv_pixel: 14166\n",
      "Img_num: 32, contain_ratio: 0.9999999964130707, f-score: 0.9062741516194759\n",
      "predict_bv_pixel: 29053, true_bv_pixel: 27879\n",
      "Img_num: 33, contain_ratio: 0.9999999943873829, f-score: 0.8791394666596188\n",
      "predict_bv_pixel: 20020, true_bv_pixel: 17817\n",
      "Img_num: 34, contain_ratio: 0.99999999515128, f-score: 0.8788526229657242\n",
      "predict_bv_pixel: 23372, true_bv_pixel: 20624\n",
      "Img_num: 35, contain_ratio: 0.9999999961186151, f-score: 0.8915471928909355\n",
      "predict_bv_pixel: 29827, true_bv_pixel: 25764\n",
      "average_dice: 0.8920487911594718\n",
      "bad img: []\n",
      "36\n",
      "average dice: 0.8920487911594718, std: 0.03802144981947143\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join('/scratch/zq415/grammar_cor/Localization','data','test_predict_c1_v2_36.pickle')\n",
    "#(all_data[current_idx][5],all_data[current_idx][6],all_data[current_idx][7],x2,y2,z2)\n",
    "all_data = load_data(data_path)\n",
    "HALF_SIDE = 128 # half of the bounding size 128\n",
    "\n",
    "all_whole_volumes = {}\n",
    "all_whole_filtered_volumes = {}\n",
    "all_whole_labels = {}\n",
    "all_right_corner = {}\n",
    "for i in range(len(all_data)):\n",
    "    all_whole_volumes[i] = all_data[i][0] - 0.5 \n",
    "    all_whole_filtered_volumes[i] = all_data[i][1] - 0.5\n",
    "    all_whole_labels[i] = all_data[i][2]\n",
    "    all_whole_labels[i][all_whole_labels[i]>0] = 1\n",
    "    all_right_corner[i] = (all_data[i][3],all_data[i][4],all_data[i][5])\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net1 = Net()\n",
    "net2 = Net()\n",
    "\n",
    "#net.apply(weight_init)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    net1 = nn.DataParallel(net1)\n",
    "    net2 = nn.DataParallel(net2)\n",
    "net1.to(device)\n",
    "net2.to(device)\n",
    "\n",
    "\n",
    "print(\"There are {} parameters in the model\".format(count_parameters(net1)))\n",
    "net1.load_state_dict(torch.load('./model/bv_seg_net128_c1_b4_e5_v1_22.pth'))\n",
    "net2.load_state_dict(torch.load('./model/bv_seg_net128_c1_b4_e4_v1_22.pth'))\n",
    "\n",
    "#test\n",
    "test_result = []\n",
    "bad_result = []\n",
    "for i in range(len(all_data)):\n",
    "    test_result.append(test(net1,net2, device, all_whole_volumes[i], all_whole_filtered_volumes[i], all_whole_labels[i],\n",
    "         all_right_corner[i], i, box_size=128, save_thresh = 0.92))\n",
    "    if test_result[i] < 0.6:\n",
    "        bad_result.append((i, test_result[i]))\n",
    "result = 0.0\n",
    "count = 0\n",
    "test_result_111 = []\n",
    "for i in range(len(test_result)):\n",
    "    if test_result[i] > 0.6:\n",
    "        result += test_result[i]\n",
    "        test_result_111.append(test_result[i])\n",
    "        count += 1\n",
    "print('average_dice: {}'.format(result/count))\n",
    "print('bad img: {}'.format(bad_result))\n",
    "print(len(test_result_111))\n",
    "print('average dice: {}, std: {}'.format(np.mean(test_result_111),np.std(test_result_111)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79759162100942826"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(test_result_111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94991437787896083"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(test_result_111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
